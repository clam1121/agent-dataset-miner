"""
NeurIPSè®ºæ–‡æ•°æ®é›†æŒ–æ˜ä¸»ç¨‹åº
ä»NeurIPSè·å–2025å¹´Spotlightå’ŒBestè®ºæ–‡ï¼Œæå–æ•°æ®é›†ä¿¡æ¯
"""

import os
import json
import logging
from pathlib import Path
from typing import List, Dict, Any

from neurips_downloader import NeurIPSDownloader
from pdf_parser import PDFParser
from llm_client import call_gpt4o_text
from prompts import (
    EXTRACT_PAPER_META_PROMPT,
    EXTRACT_DATASET_NAMES_PROMPT,
    EXTRACT_DATASET_DETAILS_PROMPT,
)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('dataset_miner_neurips.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class NeurIPSDatasetMiner:
    """NeurIPSæ•°æ®é›†ä¿¡æ¯æŒ–æ˜å™¨"""
    
    def __init__(self, output_file: str = "outputs/dataset_neurips_results.jsonl"):
        """
        åˆå§‹åŒ–
        
        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        self.output_file = Path(output_file)
        self.output_file.parent.mkdir(parents=True, exist_ok=True)
        
        # æ•°æ®é›†IDè®¡æ•°å™¨
        self.dataset_counter = 0
        
    def extract_json_from_response(self, response: str) -> Any:
        """
        ä»LLMå“åº”ä¸­æå–JSON
        
        Args:
            response: LLMè¿”å›çš„æ–‡æœ¬
            
        Returns:
            è§£æåçš„JSONå¯¹è±¡
        """
        try:
            # å°è¯•ç›´æ¥è§£æ
            return json.loads(response)
        except:
            pass
        
        # å°è¯•æå–```json```å—
        import re
        json_match = re.search(r'```json\s*(.*?)\s*```', response, re.DOTALL)
        if json_match:
            try:
                return json.loads(json_match.group(1))
            except:
                pass
        
        # å°è¯•æå–{}å—
        json_match = re.search(r'\{.*\}', response, re.DOTALL)
        if json_match:
            try:
                return json.loads(json_match.group(0))
            except:
                pass
        
        logger.error(f"æ— æ³•è§£æJSONå“åº”: {response[:200]}")
        return None
    
    def process_paper(self, pdf_path: str, category: str, paper_info: dict) -> List[Dict]:
        """
        å¤„ç†å•ç¯‡è®ºæ–‡ï¼Œæå–æ•°æ®é›†ä¿¡æ¯
        
        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
            category: è®ºæ–‡ç±»åˆ«
            paper_info: è®ºæ–‡åŸºæœ¬ä¿¡æ¯
            
        Returns:
            æ•°æ®é›†ä¿¡æ¯åˆ—è¡¨
        """
        logger.info(f"å¼€å§‹å¤„ç†è®ºæ–‡: {paper_info['title']}")
        
        results = []
        
        try:
            # 1. è§£æPDF
            parser = PDFParser(pdf_path)
            summary_text, urls = parser.get_summary_text(max_chars=25000)
            
            logger.info(f"æå–æ–‡æœ¬é•¿åº¦: {len(summary_text)}, URLæ•°é‡: {len(urls)}")
            
            # 2. æå–è®ºæ–‡å…ƒä¿¡æ¯
            logger.info("æ­¥éª¤1: æå–è®ºæ–‡å…ƒä¿¡æ¯...")
            meta_prompt = EXTRACT_PAPER_META_PROMPT.format(text=summary_text[:15000])
            meta_response = call_gpt4o_text(meta_prompt)
            paper_meta = self.extract_json_from_response(meta_response)
            
            if not paper_meta:
                logger.warning("è®ºæ–‡å…ƒä¿¡æ¯æå–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                paper_meta = {
                    "title": paper_info.get('title', 'Unknown'),
                    "authors": [],
                    "venue": f"NeurIPS {category}",
                    "year": "2025",
                    "url": paper_info.get('url', ''),
                    "is_fellow": "false"
                }
            
            # è¡¥å……NeurIPS URL
            if not paper_meta.get('url'):
                paper_meta['url'] = paper_info.get('url', '')
            
            # æ ‡è®°æ˜¯å¦ä¸ºè·å¥–è®ºæ–‡
            if category == 'best':
                paper_meta['is_fellow'] = "true"
            
            # 3. æå–æ•°æ®é›†åç§°
            logger.info("æ­¥éª¤2: æå–æ•°æ®é›†åç§°...")
            dataset_names_prompt = EXTRACT_DATASET_NAMES_PROMPT.format(text=summary_text)
            dataset_names_response = call_gpt4o_text(dataset_names_prompt)
            dataset_names_json = self.extract_json_from_response(dataset_names_response)
            
            dataset_names = []
            if dataset_names_json and 'datasets' in dataset_names_json:
                dataset_names = dataset_names_json['datasets']
            
            if not dataset_names:
                logger.warning("æœªæ‰¾åˆ°æ•°æ®é›†ï¼Œè·³è¿‡æ­¤è®ºæ–‡")
                parser.close()
                return results
            
            logger.info(f"æ‰¾åˆ° {len(dataset_names)} ä¸ªæ•°æ®é›†: {dataset_names}")
            
            # 4. ä¸ºæ¯ä¸ªæ•°æ®é›†æå–è¯¦ç»†ä¿¡æ¯
            for idx, dataset_name in enumerate(dataset_names, 1):
                logger.info(f"æ­¥éª¤3.{idx}: æå–æ•°æ®é›† '{dataset_name}' çš„è¯¦ç»†ä¿¡æ¯...")
                
                try:
                    # æå–æ•°æ®é›†è¯¦ç»†ä¿¡æ¯
                    details_prompt = EXTRACT_DATASET_DETAILS_PROMPT.format(
                        dataset_name=dataset_name,
                        text=summary_text,
                        urls="\n".join(urls)
                    )
                    details_response = call_gpt4o_text(details_prompt)
                    details = self.extract_json_from_response(details_response)
                    
                    if not details:
                        logger.warning(f"æ•°æ®é›† '{dataset_name}' è¯¦ç»†ä¿¡æ¯æå–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                        details = {
                            "content": f"A dataset mentioned in the paper: {dataset_name}",
                            "type": ["unspecified"],
                            "domain": ["unspecified"],
                            "fields": ["unspecified"],
                            "dataset_link": "",
                            "platform": ""
                        }
                    
                    # ç»„è£…ç»“æœ
                    self.dataset_counter += 1
                    dataset_record = {
                        "dataset id": str(self.dataset_counter).zfill(3),
                        "name": dataset_name,
                        "dataset describe": {
                            "content": details.get("content", ""),
                            "type": details.get("type", ["unspecified"]),
                            "domain": details.get("domain", ["unspecified"]),
                            "fields": details.get("fields", ["unspecified"])
                        },
                        "paper_refs": paper_meta,
                        "dataset link": details.get("dataset_link", ""),
                        "platform": details.get("platform", "")
                    }
                    
                    results.append(dataset_record)
                    logger.info(f"âœ“ æ•°æ®é›† '{dataset_name}' å¤„ç†å®Œæˆ")
                    
                except Exception as e:
                    logger.error(f"å¤„ç†æ•°æ®é›† '{dataset_name}' æ—¶å‡ºé”™: {str(e)}")
                    continue
            
            parser.close()
            
        except Exception as e:
            logger.error(f"å¤„ç†è®ºæ–‡æ—¶å‡ºé”™: {str(e)}", exc_info=True)
        
        return results
    
    def save_results(self, results: List[Dict]):
        """
        ä¿å­˜ç»“æœåˆ°JSONLæ–‡ä»¶ï¼ˆç«‹å³å†™å…¥ç£ç›˜ï¼‰
        
        Args:
            results: æ•°æ®é›†ä¿¡æ¯åˆ—è¡¨
        """
        if not results:
            logger.info("æœ¬æ¬¡æ— æ•°æ®é›†éœ€è¦ä¿å­˜")
            return
            
        try:
            with open(self.output_file, 'a', encoding='utf-8') as f:
                for record in results:
                    f.write(json.dumps(record, ensure_ascii=False) + '\n')
                    f.flush()  # ç«‹å³å†™å…¥ç£ç›˜
                    logger.info(f"  âœ“ å·²ä¿å­˜æ•°æ®é›†: {record['name']} (ID: {record['dataset id']})")
            logger.info(f"âœ… æˆåŠŸä¿å­˜ {len(results)} æ¡è®°å½•åˆ° {self.output_file}")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {str(e)}")
    
    def run(self, year: int = 2025):
        """
        è¿è¡Œä¸»æµç¨‹
        
        Args:
            year: NeurIPSå¹´ä»½
        """
        logger.info(f"=== å¼€å§‹æŒ–æ˜NeurIPS {year}æ•°æ®é›†ä¿¡æ¯ ===")
        logger.info(f"ç›®æ ‡: Spotlightå’ŒBestè®ºæ–‡")
        
        # åˆå§‹åŒ–ä¸‹è½½å™¨
        downloader = NeurIPSDownloader(year=year, temp_dir="temp")
        
        total_papers = 0
        total_datasets = 0
        
        try:
            # æµå¼å¤„ç†ï¼šä¸‹è½½ä¸€ç¯‡å¤„ç†ä¸€ç¯‡
            for pdf_path, category, paper_info in downloader.download_and_process_papers():
                total_papers += 1
                
                logger.info(f"\n{'='*60}")
                logger.info(f"å¤„ç†ç¬¬ {total_papers} ç¯‡è®ºæ–‡ [{category.upper()}]")
                logger.info(f"æ ‡é¢˜: {paper_info['title']}")
                logger.info(f"æ¥æº: {paper_info.get('source', 'N/A')}")
                logger.info(f"{'='*60}")
                
                try:
                    # å¤„ç†è®ºæ–‡
                    results = self.process_paper(pdf_path, category, paper_info)
                    
                    # ç«‹å³ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
                    if results:
                        logger.info(f"\nğŸ“ æ­£åœ¨ä¿å­˜æå–çš„æ•°æ®é›†ä¿¡æ¯...")
                        self.save_results(results)
                        total_datasets += len(results)
                        logger.info(f"âœ“ æœ¬ç¯‡è®ºæ–‡å®Œæˆï¼šæå–å¹¶ä¿å­˜äº† {len(results)} ä¸ªæ•°æ®é›†")
                    else:
                        logger.info(f"âš ï¸ æœ¬ç¯‡è®ºæ–‡æœªæå–åˆ°æ•°æ®é›†")
                    
                except Exception as e:
                    logger.error(f"âŒ å¤„ç†è®ºæ–‡å¤±è´¥: {str(e)}", exc_info=True)
                
                finally:
                    # åˆ é™¤PDFæ–‡ä»¶é‡Šæ”¾ç©ºé—´
                    try:
                        if os.path.exists(pdf_path):
                            os.remove(pdf_path)
                            logger.info(f"å·²åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {pdf_path}")
                    except Exception as e:
                        logger.warning(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {str(e)}")
            
            logger.info(f"\n{'='*60}")
            logger.info(f"=== å¤„ç†å®Œæˆ ===")
            logger.info(f"æ€»è®ºæ–‡æ•°: {total_papers}")
            logger.info(f"æ€»æ•°æ®é›†æ•°: {total_datasets}")
            logger.info(f"è¾“å‡ºæ–‡ä»¶: {self.output_file}")
            logger.info(f"{'='*60}")
            
        except Exception as e:
            logger.error(f"è¿è¡Œè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}", exc_info=True)


def main():
    """ä¸»å‡½æ•°"""
    miner = NeurIPSDatasetMiner(output_file="outputs/dataset_neurips_results.jsonl")
    miner.run(year=2025)


if __name__ == "__main__":
    main()

